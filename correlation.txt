suppose this. I have a protein, and two models, one (L) that predicts which mutations to introduce to improve a certain metric and one (B) that predicts the metric based on a given sequence. What I am doing is this. original -> L -> l1, l2, ... -> B -> m1, m2, ... -> select best  However by doing so I am throwing all the information contained in the ones that are not the best. O instead I want to do this. From L, I generate l1...N, I profile the mutation. For each position, I get what it the general and specific (are these good terms?) mutation propensities p(i) = (#mutated at pos i) / N  and p(i, aa) = (#mutated at pos i to amino acid aa) / (#mutated at pos i). Then I generate sequences randomly based on these statistics e.g. 3 mutations per sequence, using p(i) and p(i, aa) as weights. Then I use B to get m for the various sequence. At this point, however, I want to create a 1 D and a 2 D matrices containing some correlation between m and mutations (1D = correlation to position i being mutated, 2D: correlation to position i mutated to aa), and then iteratively feed back this correlative information into p(i) and p(i, aa) to obtain p(i; c), p(i, aa; c) that will then be used to generate new sequences. For example, if L predicts mutation at pos 3 and this always leads to a worse metric (one has to define worse as well, higher or lower) then we simply don't want to sample from that. Of course we don't have infinite examples of mutated 3, so the number of samples/uncertainty has to be a factor in the definition of p(i; c) and p(i, aa; c). Can you please help me in constructing c, p(i; c), p(i, aa; c)?