#!/bin/bash
#
# SLURM Submission Script for BioPipelines
#
# This script submits a BioPipelines job to SLURM, mimicking the functionality
# of going to job composer and running the job with the suggested job name.
#
# Usage:
#     ./submit.sh [pipeline_name] [job_name]
#
# If no arguments provided, will look for the most recent pipeline output.
#

module load miniforge3
# miniforge3 automatically sources conda.sh
# source /apps/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/mamba-23.11.0-0-334ztq7i4mzu762ew2x3kbbrrorhe6eg/etc/profile.d/conda.sh


set -e  # Exit on any error

# Activate or create biopipelines conda environment
if mamba env list | grep -q "^biopipelines "; then
    echo "Activating biopipelines environment..."
    mamba activate biopipelines
else
    echo "biopipelines environment not found. Creating it..."

    # Get the directory where submit script is located (biopipelines root)
    SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
    ENV_FILE="$SCRIPT_DIR/Environments/biopipelines.yaml"

    if [ -f "$ENV_FILE" ]; then
        echo "Using environment specification: $ENV_FILE"
        mamba env create -f "$ENV_FILE"
    else
        echo "Warning: biopipelines.yaml not found, creating minimal environment"
        mamba create -n biopipelines -y python pyyaml pandas requests numpy
    fi

    echo "Activating newly created biopipelines environment..."
    mamba activate biopipelines
fi
echo ""

# Function to print usage
usage() {
    echo "Usage: $0 [pipeline_script]"
    echo ""
    echo "Run a BioPipelines script and submit the generated job to SLURM"
    echo ""
    echo "Arguments:"
    echo "  pipeline_script  Path to pipeline script (optional, defaults to pipeline.py)"
    echo ""
    echo "Examples:"
    echo "  $0                                           # Run pipeline.py"
    echo "  $0 pipeline.py                               # Run pipeline.py"
    echo "  $0 ExamplePipelines/ligandmpnn_boltz2.py    # Run specific pipeline"
    echo ""
}

# Function to run pipeline and execute sbatch commands
run_pipeline_and_submit() {
    local pipeline_script="$1"

    echo "Running pipeline: $pipeline_script"
    echo ""

    # Run the pipeline and capture output
    local pipeline_output=$(python "$pipeline_script" 2>&1)
    local exit_code=$?

    # Show pipeline output
    echo "$pipeline_output"
    echo ""

    if [ $exit_code -ne 0 ]; then
        echo "Pipeline execution completed with exit code $exit_code"
        echo "Note: This is expected for dummy/test pipelines"
        echo ""
    fi

    # Check if this is a multi-batch pipeline by looking for slurm_batch*.sh files
    # Extract runtime directory from pipeline output
    local runtime_dir=$(echo "$pipeline_output" | grep -o '/[^ ]*/RunTime' | head -1)

    if [ -n "$runtime_dir" ] && [ -d "$runtime_dir" ]; then
        # Check for batch files
        local batch_files=$(ls "$runtime_dir"/slurm_batch*.sh 2>/dev/null | sort -V)

        if [ -n "$batch_files" ]; then
            # Multi-batch pipeline
            echo ""
            echo "Detected multi-batch pipeline"
            echo "Submitting jobs with dependencies..."
            echo "=========================================="
            echo ""

            local job_count=0
            local last_job_id=""

            for batch_file in $batch_files; do
                job_count=$((job_count + 1))

                # Replace <JOBID> placeholder with previous job ID
                if [ -n "$last_job_id" ]; then
                    echo "Updating $batch_file with dependency on job $last_job_id"
                    sed -i "s/<JOBID>/$last_job_id/g" "$batch_file"
                fi

                # Extract job name from batch file or use default
                local batch_num=$(basename "$batch_file" | grep -o '[0-9]\+')
                local job_name=$(echo "$pipeline_output" | grep "Job:" | head -1 | sed 's/.*Job: //')

                # Submit and capture job ID
                echo "Submitting batch $batch_num..."
                local submit_output=$(sbatch --parsable "$batch_file" 2>&1)
                local submit_exit=$?

                if [ $submit_exit -eq 0 ]; then
                    last_job_id="$submit_output"
                    echo "✓ Batch $batch_num submitted: Job ID $last_job_id"
                else
                    echo "✗ Failed to submit batch $batch_num"
                    echo "$submit_output"
                    return 1
                fi
                echo ""
            done

            echo "=========================================="
            echo "All batches submitted. Total: $job_count"
            echo "Job dependency chain: $batch_files"
            return 0
        fi
    fi

    # Single batch pipeline (original behavior)
    local sbatch_commands=$(echo "$pipeline_output" | grep "^sbatch ")

    if [ -z "$sbatch_commands" ]; then
        echo "No sbatch commands found in pipeline output"
        return 1
    fi

    echo ""
    echo "Submitting jobs..."
    echo "=========================================="
    echo ""

    # Execute each sbatch command
    local job_count=0
    while IFS= read -r sbatch_cmd; do
        if [ -n "$sbatch_cmd" ]; then
            job_count=$((job_count + 1))
            echo "Job $job_count:"
            echo "$sbatch_cmd"
            eval "$sbatch_cmd" || true
            echo ""
        fi
    done <<< "$sbatch_commands"

    echo "=========================================="
    echo "All jobs submitted. Total: $job_count"
}

# Main execution
main() {
    local pipeline_script="$1"

    echo "BioPipelines SLURM Submission Script"
    echo "===================================="
    echo ""

    # Handle help request
    if [ "$1" = "-h" ] || [ "$1" = "--help" ]; then
        usage
        return 0
    fi

    # Check if pandas is available
    #echo "Checking environment dependencies..."
    #if ! python -c "import pandas" >/dev/null 2>&1; then
    #    echo ""
    #    echo "Error: BioPipelines requires pandas to run. It is usually enough to run:"
    #    echo "   module load mamba"
    #    echo "before running the submit script. Otherwise you might have to:"
    #    echo "A. Install pandas first with pip install pandas, or"
    #    echo "B. Activate a conda environment that contains pandas"
    #    return 1
    #fi
    #echo "✓ pandas available"
    #echo ""

    # Default to pipeline.py if no script specified
    if [ -z "$pipeline_script" ]; then
        pipeline_script="pipeline.py"
    fi

    # Check if pipeline script exists
    if [ ! -f "$pipeline_script" ]; then
        echo "Error: Pipeline script not found: $pipeline_script"
        echo ""
        echo "Available options:"
        echo "  ./submit.sh                     # Run pipeline.py"
        echo "  ./submit.sh pipeline.py         # Run specific pipeline script"
        echo "  ./submit.sh ExamplePipelines/ligandmpnn_boltz2.py"
        return 1
    fi

    # Run the pipeline and submit jobs
    run_pipeline_and_submit "$pipeline_script"
}

# Run main function with all arguments
main "$@"