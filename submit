#!/bin/bash
# Copyright (c) 2026 Gianluca Quargnali @ LOCBP (https://www.locbp.com/) University of Zurich Switzerland
#
# This software is freely available for use, modification, and redistribution.

#
# SLURM Submission Script for BioPipelines
#
# This script submits a BioPipelines job to SLURM, mimicking the functionality
# of going to job composer and running the job with the suggested job name.
#
# Usage:
#     ./submit.sh <pipeline_script>
#

# Get the directory where submit script is located (biopipelines root)
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
CONFIG_FILE="$SCRIPT_DIR/config.yaml"
# Read cluster config from config.yaml (before Python is available)
# env_manager: "mamba" or "conda"
ENV_MANAGER=$(grep '^\s*env_manager:' "$CONFIG_FILE" 2>/dev/null | head -1 | sed 's/.*env_manager:\s*["'\'']\{0,1\}\([a-zA-Z]*\)["'\'']\{0,1\}.*/\1/' | tr -d '[:space:]')
if [ -z "$ENV_MANAGER" ]; then
    echo "ERROR: env_manager not found in config.yaml (cluster section)"
    echo "Please add 'env_manager: mamba' (or conda) under 'cluster:' in $CONFIG_FILE"
    exit 1
fi

# slurm_modules: read as space-separated list (empty list means no modules to load)
SLURM_MODULES=$(awk '/^\s*slurm_modules:/{found=1; next} found && /^\s*-/{gsub(/^\s*-\s*/, ""); gsub(/"/, ""); printf "%s ", $0; next} found && /^\S/{exit}' "$CONFIG_FILE" 2>/dev/null | xargs)

# Load modules (skip if empty)
if [ -n "$SLURM_MODULES" ]; then
    module load $SLURM_MODULES
fi

# Initialize environment manager for current shell session
if [ "$ENV_MANAGER" = "conda" ]; then
    eval "$(conda shell.bash hook)"
else
    eval "$($ENV_MANAGER shell hook --shell bash)"
fi

set -e  # Exit on any error

# Check if biopipelines env exists
if $ENV_MANAGER env list | grep -qw "biopipelines"; then
    echo "Activating biopipelines environment..."
    $ENV_MANAGER activate biopipelines
else
    echo "ERROR: biopipelines environment not found."
    echo ""
    echo "Please install it first (see Docs/UserManual.md):"
    echo "  $ENV_MANAGER env create -f Environments/biopipelines.yaml"
    echo "  $ENV_MANAGER activate biopipelines"
    echo "  pip install -e ."
    exit 1
fi
echo ""

# Function to print usage
usage() {
    echo "Usage: $0 <pipeline_script>"
    echo ""
    echo "Run a BioPipelines script and submit the generated job to SLURM"
    echo ""
    echo "Arguments:"
    echo "  pipeline_script  Path to pipeline script"
    echo ""
    echo "Examples:"
    echo "  $0 pipeline.py                               # Run pipeline.py"
    echo "  $0 ExamplePipelines/ligandmpnn_boltz2.py    # Run specific pipeline"
    echo ""
}

# Function to run pipeline and execute sbatch commands
run_pipeline_and_submit() {
    local pipeline_script="$1"

    echo "Running pipeline: $pipeline_script"
    echo ""

    # Run the pipeline and capture output
    local pipeline_output=$(python "$pipeline_script" 2>&1)
    local exit_code=$?

    # Show pipeline output
    echo "$pipeline_output"
    echo ""

    if [ $exit_code -ne 0 ]; then
        echo "Pipeline execution completed with exit code $exit_code"
        echo "Note: This is expected for dummy/test pipelines"
        echo ""
    fi

    # Extract ALL runtime directories from pipeline output
    local runtime_dirs=$(echo "$pipeline_output" | grep -o '/[^ ]*/RunTime' | sort -u)

    if [ -z "$runtime_dirs" ]; then
        echo "No runtime directories found in pipeline output"
        # Fall back to checking for sbatch commands (backward compatibility)
        local sbatch_commands=$(echo "$pipeline_output" | grep "^sbatch ")
        if [ -n "$sbatch_commands" ]; then
            echo ""
            echo "Submitting jobs..."
            echo "=========================================="
            echo ""

            local job_count=0
            while IFS= read -r sbatch_cmd; do
                if [ -n "$sbatch_cmd" ]; then
                    job_count=$((job_count + 1))
                    echo "Job $job_count:"
                    echo "$sbatch_cmd"
                    eval "$sbatch_cmd" || true
                    echo ""
                fi
            done <<< "$sbatch_commands"

            echo "=========================================="
            echo "All jobs submitted. Total: $job_count"
            return 0
        else
            return 1
        fi
    fi

    echo ""
    echo "Found $(echo "$runtime_dirs" | wc -l) pipeline(s) to submit"
    echo "=========================================="
    echo ""

    local total_jobs_submitted=0

    # Process each runtime directory
    while IFS= read -r runtime_dir; do
        if [ -z "$runtime_dir" ] || [ ! -d "$runtime_dir" ]; then
            continue
        fi

        # Extract pipeline name from runtime directory
        local pipeline_name=$(basename "$(dirname "$runtime_dir")")
        echo "Processing pipeline: $pipeline_name"
        echo "Runtime directory: $runtime_dir"
        echo ""

        # Check for batch files (multi-batch pipeline)
        local batch_files=$(ls "$runtime_dir"/slurm_batch*.sh 2>/dev/null | sort -V)

        if [ -n "$batch_files" ]; then
            # Multi-batch pipeline
            echo "Detected multi-batch pipeline"
            echo "Submitting jobs with dependencies..."
            echo "------------------------------------------"
            echo ""

            local job_count=0
            local last_job_id=""

            for batch_file in $batch_files; do
                job_count=$((job_count + 1))

                # Replace <JOBID> placeholder with previous job ID
                if [ -n "$last_job_id" ]; then
                    echo "Updating $batch_file with dependency on job $last_job_id"
                    sed -i "s/<JOBID>/$last_job_id/g" "$batch_file"
                    # Verify replacement worked
                    if grep -q "<JOBID>" "$batch_file"; then
                        echo "Warning: Failed to replace <JOBID> placeholder in $batch_file"
                    fi
                fi

                # Extract batch number for job naming
                local batch_num=$(basename "$batch_file" | grep -o '[0-9]\+')
                local job_name="${pipeline_name}_batch${batch_num}"
                local output_path="$runtime_dir/job_batch${batch_num}.out"

                # Submit and capture job ID
                echo "Submitting batch $batch_num..."
                echo "  Job name: $job_name"
                echo "  Output: $output_path"
                if [ -n "$last_job_id" ]; then
                    echo "  Dependency: afterok:$last_job_id"
                fi

                local submit_output=$(sbatch --parsable --job-name="$job_name" --output="$output_path" "$batch_file" 2>&1)
                local submit_exit=$?

                # Check for both exit code and error messages in output
                if [ $submit_exit -eq 0 ] && ! echo "$submit_output" | grep -qi "error\|invalid\|failed"; then
                    last_job_id="$submit_output"
                    echo "✓ Batch $batch_num submitted: Job ID $last_job_id"
                    total_jobs_submitted=$((total_jobs_submitted + 1))
                else
                    echo "✗ Failed to submit batch $batch_num"
                    echo "   Error: $submit_output"
                    return 1
                fi
                echo ""
            done

            echo "Pipeline $pipeline_name: $job_count batch(es) submitted"
            echo ""
        else
            # Single batch pipeline - look for slurm.sh
            local slurm_script="$runtime_dir/slurm.sh"
            if [ -f "$slurm_script" ]; then
                echo "Detected single-batch pipeline"
                echo "Submitting job..."
                echo "------------------------------------------"
                echo ""

                # Extract job name from pipeline output
                local job_name=$(echo "$pipeline_output" | grep -A1 "$runtime_dir" | grep -o "Job: .*" | sed 's/Job: //' | head -1)
                if [ -z "$job_name" ]; then
                    job_name="$pipeline_name"
                fi

                local output_path="$runtime_dir/slurm.out"
                local submit_output=$(sbatch --job-name="$job_name" --output "$output_path" --parsable "$slurm_script" 2>&1)
                local submit_exit=$?

                # Check for both exit code and error messages in output
                if [ $submit_exit -eq 0 ] && ! echo "$submit_output" | grep -qi "error\|invalid\|failed"; then
                    echo "✓ Job submitted: Job ID $submit_output"
                    total_jobs_submitted=$((total_jobs_submitted + 1))
                else
                    echo "✗ Failed to submit job"
                    echo "   Error: $submit_output"
                    return 1
                fi
                echo ""
            else
                echo "Warning: No slurm script found in $runtime_dir"
                echo ""
            fi
        fi
    done <<< "$runtime_dirs"

    echo "=========================================="
    echo "All pipelines submitted. Total jobs: $total_jobs_submitted"
}

# Main execution
main() {
    local pipeline_script="$1"

    echo "BioPipelines SLURM Submission Script"
    echo "===================================="
    echo ""

    # Handle help request
    if [ "$1" = "-h" ] || [ "$1" = "--help" ]; then
        usage
        return 0
    fi

    # Require a pipeline script argument
    if [ -z "$pipeline_script" ]; then
        echo "Error: No pipeline script specified"
        echo ""
        usage
        return 1
    fi

    # Check if pipeline script exists
    if [ ! -f "$pipeline_script" ]; then
        echo "Error: Pipeline script not found: $pipeline_script"
        return 1
    fi

    # Run the pipeline and submit jobs
    run_pipeline_and_submit "$pipeline_script"
}

# Run main function with all arguments
main "$@"